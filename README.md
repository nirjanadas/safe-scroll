<!-- ===================== -->
<!-- SafeScroll App Banner -->
<!-- ===================== -->

<p align="center">
  <img src="assets/safescroll_overview.png" width="100%" />
</p>

<p align="center">
  <img src="assets/safescroll_moderation.png" width="100%" />
</p>

---

# ğŸ›¡ï¸ SafeScroll â€” AI-Powered Social Media Safety Auditor  
**Making your social feed safer â€” one scroll at a time.**

SafeScroll is an **AI-powered Trust & Safety auditing system** that simulates how modern social media platforms internally detect and respond to harmful content and unsafe user behavior.

It demonstrates how **multi-agent GenAI systems** can be applied to:
- content moderation
- underage safety
- grooming detection
- policy enforcement
- human-readable safety reporting

This project is built for **research, demos, and resume-ready showcasing** of real-world AI safety engineering.

---

 
## âœ¨ Features

- Multi-agent GenAI workflow:
  - Underage Risk Agent
  - Content Safety Agent (bullying, self-harm, grooming, substance)
  - Interaction/Grooming Agent
  - Policy Violation Agent
  - Safety Report Generator
- Streamlit dashboard to:
  - Select a user
  - Inspect posts + DMs
  - Run a complete safety audit
  - View structured JSON outputs
  - Visualise risk levels with a bar chart
  - Download the final report as a `.txt` file

---

## ğŸ—‚ Folder structure

```text
 safe-scroll/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ safescroll_overview.png
â”‚   â””â”€â”€ safescroll_moderation.png
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ interactions.csv
â”‚   â”œâ”€â”€ posts.csv
â”‚   â””â”€â”€ users.csv
â”œâ”€â”€ policies/
â”‚   â””â”€â”€ safety_policies.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ agents.py
â”œâ”€â”€ app.py
â”œâ”€â”€ generate_synthetic_data.py
â””â”€â”€ requirements.txt
```
