# ğŸ›¡ï¸ AI Safety & Risk Auditor for Social Platforms

This project is a small simulation of an **internal Trust & Safety tool** for a social media platform.

It uses a **multi-agent LLM pipeline** on top of synthetic company data (users, posts, DMs) to:

- Analyse user posts and private messages
- Detect underage risk / misrepresentation
- Detect bullying and harassment
- Detect self-harm / mental health risk
- Detect grooming / sexual exploitation patterns
- Detect substance abuse promotion
- Map the findings to simplified **safety policies**
- Generate a **user-level safety report** with an overall risk score and recommended action

All data in this project is **synthetic** and for **research/demo purposes only**.

---

## âœ¨ Features

- Multi-agent GenAI workflow:
  - Underage Risk Agent
  - Content Safety Agent (bullying, self-harm, grooming, substance)
  - Interaction/Grooming Agent
  - Policy Violation Agent
  - Safety Report Generator
- Streamlit dashboard to:
  - Select a user
  - Inspect posts + DMs
  - Run a complete safety audit
  - View structured JSON outputs
  - Visualise risk levels with a bar chart
  - Download the final report as a `.txt` file

---

## ğŸ—‚ Folder structure

```text
 safe-scroll/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ safescroll_overview.png
â”‚   â””â”€â”€ safescroll_moderation.png
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ interactions.csv
â”‚   â”œâ”€â”€ posts.csv
â”‚   â””â”€â”€ users.csv
â”œâ”€â”€ policies/
â”‚   â””â”€â”€ safety_policies.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ agents.py
â”œâ”€â”€ app.py
â”œâ”€â”€ generate_synthetic_data.py
â””â”€â”€ requirements.txt
```
